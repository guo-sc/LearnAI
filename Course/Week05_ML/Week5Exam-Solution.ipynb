{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营第五周(机器学习基础)考试-参考答案\n",
    "#### 参考答案说明:\n",
    "\n",
    "- 来源：来自于网络搜索，笔记整理，当期及往期优秀同学试卷等途径\n",
    "- 使用：该答案仅供参考，非唯一固定答案，欢迎同学批评，指正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输入空间：所有输入可能的取值的集合\n",
    "- 输出空间：所有输出可能的取值的集合\n",
    "- 特征空间：每个具体的输入数据也叫一个实例通常由特征向量表示.所有特征向量存在的空间\n",
    "- 假设空间：由输入空间到输出空间映射的集合就是假设空间.假设空间的确定意味着学习范围的确定. \n",
    "\n",
    "- 参数空间：\n",
    "    - 非概率模型中，使得输入空间$X$中的变量$x$在通过假设空间$\\mathcal{F}$中的函数$f$后得到输出空间$Y$中的$y$成立的函数f的所有参数值（向量）组成的空间\n",
    "    - 概率模型中，使得输出空间$Y$中的变量$y$，在满足输入空间$X$中的变量$x$的条件下，使得$P(y|x)$成立的所有概率模型的参数向量组成的空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 损失函数：用来度量预测值$f(X)$与真实值$Y$相差的错误程度的函数，即$f(X)$和$Y$的非负实值函数，通常记作:$L(Y,f(X))$   \n",
    "\n",
    "\n",
    "- 常用的损失函数：\n",
    "    - 0-1损失函数：\n",
    "    $$L(Y, f(X)) = \\left\\{\\begin{aligned} & 1, Y \\neq f(X) \\\\ & 0, Y = f(X)\\end{aligned}\\right.$$ \n",
    "    - 平方损失函数\n",
    "    $$ L(Y, f(X)) = ( Y - f(X))^{2}$$ \n",
    "    - 绝对值损失函数 \n",
    "    $$ L(Y, f(X)) = |Y - f(X)|$$ \n",
    "    - 对数损失函数\n",
    "    $$ L(Y, P(Y|X)) = - \\log P(Y|X) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经验风险最小化的公式定义为：\n",
    "$$R_{emp}(f)=\\frac {1} {N} \\sum_{i=1}^N L(y_i,f(x_i)) $$\n",
    "\n",
    "结构风险最小化的公式定义为：\n",
    "$$R_{srm}(f)=\\frac {1} {N} \\sum_{i=1}^N L(y_i,f(x_i)) + \\lambda J(f) $$\n",
    "\n",
    "可以看到，结构风险在经验风险的基础上加上了表示模型复杂度的正则项或惩罚项，防止在数据样本小或者模型能力强的情况下出现过拟合。\n",
    "$J(f)$为模型的复杂度，模型 $f$ 越复杂，$J(f)$越大。$\\lambda>=0$是权衡经验风险和模型复杂度的系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "范数(norm)可以简单的理解为向量的长度或大小，或者向量到零点（坐标轴原点）的距离，或者相应的两个向量点之间的距离。常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。其有着非负性；齐次性；三角不等式三个特点。   \n",
    "范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则\n",
    "\n",
    "- L1范数：向量各元素绝对值之和\n",
    "    $$\\lVert x \\lVert _{1} = \\sum_{i}|x_{i}|$$\n",
    "- L2范数：向量各元素绝对值的平方和后再开方\n",
    "    $$\\lVert x\\lVert _{2} = \\sqrt{\\sum_{i}x_{i}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请写出软件支持向量机原始优化问题，并解释其中松弛变量$\\xi_i$在不同取值时相应的样本分布情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性支持向量机（软间隔支持向量机）：给定线性不可分训练数据集，通过求解凸二次规划问题  \n",
    "\\begin{align*}  \\\\ & \\min_{\\mathbf{w},b,\\xi} \\quad \\dfrac{1}{2} \\| \\mathbf{w} \\|^{2} + C \\sum_{i=1}^{N} \\xi_{i}\n",
    "\\\\ & s.t. \\quad y_{i} \\left( \\mathbf{w} \\cdot \\mathbf{x}_{i} + b \\right) \\geq 1 - \\xi_{i}\n",
    "\\\\ & \\xi_{i} \\geq 0, \\quad i=1,2, \\cdots, N \\end{align*}  \n",
    "松弛变量$\\xi_i$的不同取值，样本的分布情况：\n",
    "\\begin{align*} \\xi_{i} \\geq 0 \\Leftrightarrow \\left\\{\n",
    "\\begin{aligned} \n",
    "\\ &  \\xi_{i}=0, x_{i}在间隔边界上;\n",
    "\\\\ & 0 < \\xi_{i} < 1, x_{i}在间隔边界与分离超平面之间;\n",
    "\\\\ & \\xi_{i}=1, x_{i}在分离超平面上;\n",
    "\\\\ & \\xi_{i}>1, x_{i}在分离超平面误分类一侧;\n",
    "\\end{aligned}\n",
    "\\right.\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 信息熵\n",
    "    - 概念：在信息论与概率统计中，熵是表示随机变量不确定性的度量\n",
    "    - 公式：\n",
    "        $$H(X) =  -\\sum_{i=1}^{n}P_{i}\\log P_{i}$$\n",
    "- 信息增益\n",
    "    - 概念：使用某特征进行信息划分集合，使用划分前后集合熵的差值（信息的不确定减少的程度）来衡量使用当前特征对于样本集合D划分效果的好坏。\n",
    "    - 公式：\n",
    "        $$g(D,A) = H(D) - H(D|A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯的公式：\n",
    "$$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$$\n",
    "其中$P(Y)$是先验概率，$P(Y|X)$是后验概率<br><br>\n",
    "\n",
    "\n",
    "理解贝叶斯思想：   \n",
    "**逆概思想**: 我们容易计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”,很明显就是M/(N+M)。而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。   \n",
    "而贝叶斯公式就是为解决这样的‘逆概率问题’而诞生的。***贝叶斯公式就是在不完全信息下，对部分位置状态使用主观概率或统计得来的先验概率。***贝叶斯公式对诱发某结果的可能性（原因）进行概率推理，即所谓‘逆概率问题’\n",
    "\n",
    "它与频率派的区别：\n",
    "  - 频率派：认为概率是客观的，是频率的极限表现。认为数据都是在某个参数（注意，这时是固定看待的常数值）下产生的，虽然不知道参数值是什么，但其一定存在，可以通过大量重复实验而估得。极大似然和置信区间这样的方法，他们更关心的是有多大的把握能找到那个固定的参数   \n",
    "  \n",
    "  \n",
    "  - 贝叶斯派：认为频率是主观的，是信心的绝佳解释。与频率派相反，其认为参数的取值是一个随机变量，是一个有关于参数取值的概率分布，这也是贝叶斯派的核心理念，其分布即先验概率分布，通过不断获取新的数据，通过MAP不断的调整刷新参数取值发生的概率，最终得到结果（概率的后验分布）。其最关心的是能覆盖住参数取值的区间，而不是某一具体值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设对于输入x分类为1和0的概率分别为：\n",
    "$$P(y = 1|x;\\theta) = h_{\\theta}(x)$$\n",
    "$$P(y = 0|x;\\theta) = 1 - h_{\\theta}(x)$$\n",
    "\n",
    "则概率函数为：\n",
    "$$P(y|x;\\theta) = (h_{\\theta})^{y} \\cdot (1 - h_{\\theta})^{1-y}$$\n",
    "\n",
    "假设样本数据有m个，并且独立，则他们的联合分布可以由各边际分布的乘积表示，则有似然函数为：\n",
    "$$L(\\theta) = \\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\\theta) = \\prod_{i=1}^{m}(h_{\\theta}(x^{(i)}))^{y^{(i)}} \\cdot (1 - h_{\\theta}(x^{(i)}))^{1-y^{(i)}}$$\n",
    "\n",
    "取对数似然函数：\n",
    "$$l(\\theta) = \\log (L(\\theta)) = \\sum_{i=1}^{m}[\\log ((h_{\\theta}(x^{(i)}))^{y^{(i)}}) + \\log ((1 - h_{\\theta}(x^{(i)}))^{1-y^{(i)}})]$$\n",
    "\n",
    "化简：\n",
    "$$l(\\theta) = \\log (L(\\theta)) = \\sum_{i=1}^{m}[ y^{(i)}\\log (h_{\\theta}(x^{(i)})) + (1-y^{(i)}) \\log ((1 - h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "为了求解最优的参数$\\theta$，则需要最大化上述的对数似然函数，而上述的对数似然函数是以和的形式展示的，取负号，并乘以一个常数项，问题则变为求最小值，转化如下：\n",
    "$$-\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log (h_{\\theta}(x^{(i)})) + (1-y^{(i)}) \\log ((1 - h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "就是逻辑回归的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min-max标准化：   \n",
    "也称为极差法，这是对原始数据的一种线性变换，使原始数据映射到[0-1]之间。本质上是指将原始数据的最大值映射成1，是最大值归一化。归一化的依据非常简单，不同变量往往量纲不同，归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。\n",
    "\n",
    "$${ X }^{ * }=\\frac{X_{i} - X_{min}}{X_{max} - X_{min}}$$   \n",
    "Z-Score(标准化)：   \n",
    "也称为标准化分数，这种方法根据原始数据的均值和标准差进行标准化，即均值为0，标准差为1,它表示的是原始值与均值之间差多少个标准差，是一个相对值，有去除量纲的作用。\n",
    "$${ X }^{ * }=\\frac{X_{i} - \\mu}{\\sigma}$$\n",
    "\n",
    "其中$\\mu$和$\\sigma$代表样本的均值和标准差，$X_{max}$为特征取值的最大值，$X_{min}$为特征取值的最小值\n",
    "\n",
    "\n",
    "两者相似点：   \n",
    "两种处理的本质都是对数据做线性变换，将数据压缩到[0,1]区间或者最大标准差之间的范围，消除特征量纲对模型训练的影响\n",
    "\n",
    "\n",
    "两者不同点：   \n",
    "1. 归一化的缩放仅跟最大和最小值相关；而标准化的缩放和每个点都有关系，通过均值和方差体现出来<br>\n",
    "2. 归一化的输出范围在0-1之间；而标准化的输出范围是与数据的标准差相关<br>\n",
    "\n",
    "\n",
    "为什么树型结构不需要做缩放?   \n",
    "因为数值缩放不影响分裂点位置,对树模型的结构不造成影响。按照特征值对数据进行排序，排序的顺序不变，那么分裂点就不会不同。\n",
    "对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。而对于归一化的数据，损失等高线是圆形，更少的迭代次数即可到达最优点。树模型不使用梯度下降，因为构建树模型相当于寻找最优分裂点，因此树模型是阶跃的，在阶跃点处不可导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 简述映射函数、核函数以及核方法的作用，列出常用核函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核函数  \n",
    "设$\\mathcal{X}$是输入空间（欧氏空间$\\mathbb{R}^{n}$的子集或离散集合），$\\mathcal{H}$是特征空间（希尔伯特空间），如果存在一个从$\\mathcal{X}$到$\\mathcal{H}$的映射\n",
    "\\begin{align*} \\\\& \\phi \\left( \\mathbf{x} \\right) : \\mathcal{X} \\to \\mathcal{H}   \\end{align*}  \n",
    "使得对所有$\\mathbf{x},\\mathbf{z} \\in \\mathcal{X}$，函数$K \\left(\\mathbf{x}, \\mathbf{z} \\right)$满足条件  \n",
    "\\begin{align*} \\\\ &  K \\left(\\mathbf{x}, \\mathbf{z} \\right) = \\phi \\left( \\mathbf{x} \\right) \\cdot \\phi \\left( \\mathbf{z} \\right)  \\end{align*}  \n",
    "则称$K \\left(\\mathbf{x}, \\mathbf{z} \\right)$为核函数，$\\phi \\left( \\mathbf{x} \\right)$为映射函数，式中$\\phi \\left( \\mathbf{x} \\right) \\cdot \\phi \\left( \\mathbf{z} \\right)$为$\\phi \\left( \\mathbf{x} \\right)$和$\\phi \\left( \\mathbf{z} \\right)$的内积。    \n",
    "常用核函数：  \n",
    "1. 多项式核函数\n",
    "\\begin{align*} \\\\& K \\left( \\mathbf{x}, \\mathbf{z} \\right) = \\left( \\mathbf{x} \\cdot \\mathbf{z} + 1 \\right)^{p} \\end{align*}   \n",
    "2. 高斯核函数  \n",
    "\\begin{align*} \\\\& K \\left( \\mathbf{x}, \\mathbf{z} \\right) = \\exp \\left( - \\dfrac{\\| \\mathbf{x} - \\mathbf{z} \\|^{2}}{2 \\sigma^{2}} \\right) \\end{align*}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
